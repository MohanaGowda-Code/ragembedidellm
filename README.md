# RAG and LLM Document Chat (Spring Boot + Java)
A **production-style Retrieval Augmented Generation (RAG)** application built using **Java, Spring Boot, and local LLM integration (Ollama)**.
This project demonstrates how to chat with your own documents (PDFs) by combining:

* Document ingestion
* Semantic search using embeddings
* Context building
* LLM-based answer generation

---

## ğŸš€ Features
* ğŸ“„ **PDF Upload API**
* ğŸ§  **Text Extraction** (Apache Tika / OCR-ready)
* âœ‚ï¸ **Chunking Strategy** for large documents
* ğŸ” **Embedding + Semantic Search** (Top-K retrieval)
* ğŸ§© **Context Builder**
* ğŸ’¬ **Chat API** (RAG pipeline)
* ğŸ¤– **LLM Integration Ready (Ollama)**

---

## ğŸ—ï¸ Architecture Flow

```
PDF Upload
   â†“
Text Extraction
   â†“
Chunking
   â†“
Embedding Storage
   â†“
Semantic Search (Top-K)
   â†“
Context Building
   â†“
Question + Context
   â†“
LLM (Ollama / OpenAI / etc.)
   â†“
Answer
```

---

## ğŸ› ï¸ Tech Stack

* **Java 17**
* **Spring Boot 3.x**
* **Spring Web / WebClient**
* **Embedding Store (In-Memory / Vector DB ready)**
* **Apache Tika** (PDF text extraction)
* **Ollama (Local LLM runtime)**
* **REST APIs**

---

## ğŸ“Œ APIs

### 1ï¸âƒ£ Upload PDF

```
POST /upload
```

Uploads a PDF file and processes it for RAG.

---

### 2ï¸âƒ£ Chat with Documents

```
POST /chat
```

**Request Body**

```json
{
  "question": "health not good"
}
```

**Response**

```text
LLM response based on document context
```

---

## LLM Integration (Ollama)

The project is designed to work with **local LLMs using Ollama**.

Current status:

* LLM service interface is implemented
* Placeholder response is returned until Ollama is running

```java
return "LLM response will come here";
```

Once Ollama is available, **no architectural changes are required** â€” only the response will be generated by the LLM.

---

## Running Locally

### Prerequisites

* Java 17+
* Maven
* Ollama (optional for now)

### Start Application

```bash
mvn spring-boot:run
```

---

## Testing

* Use **Postman** or **cURL**
* Upload a PDF
* Call `/chat` with a question

---

## ğŸ—ºï¸ Roadmap

* [ ] Enable Ollama LLM responses
* [ ] Streaming responses
* [ ] Source citations
* [ ] Multiple document support
* [ ] Vector DB integration (FAISS / Pinecone / Weaviate)
* [ ] Authentication & rate limiting

---

## ğŸ‘¤ Author

**Mohana**  
Lead Consultant â€“ Java & Microservices Engineer  
Exploring AI, RAG, and LLM Integration with Spring Boot

---
## â­ Notes

This project is built as a **real-world GenAI backend reference** for Java developers looking to move into **AI + Microservices**.

Contributions and improvements are welcome.
